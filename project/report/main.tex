\documentclass[a4paper, 12pt]{article}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{physics}

\captionsetup{width=0.8 \linewidth}

\begin{document}

\title{Relations between entropy and file size in common image compression algorithms}
\author{Vincent Ud√©n\\udenv@student.chalmers.se}
\date{Februari - 2023}

\maketitle

\section{Introduction}
In the digital age, information has taken center stage in most industries across society. Scientific research has moved into an epoch of data-driven analysis, fitting models to increasingly vast and detailed amounts of data. It is estimated that we store a total of 295 exabytes ($295 \cdot 10^9$ gigabytes) across devices, digital and analog. This doesn't even encompass the totality of that information, since a large part of it is stored in a compressed format. In fact, parts of everyday life wouldn't be possible without data compression. Streaming an HD video in real time would require $1920 \cdot 1080 \cdot 3\cdot 24 = 149.3$ megabytes/s of bandwidth, a far cry from the average internet speed of XXX megabytes/s in Sweden. This report investigates how entropy and storage size change under a few common image compression algorithms, a part of vital video compression algorithms.

\section{Method}

\section{Results}

\section{Discussion}

% Data on earth
% https://news.usc.edu/29360/how-much-information-is-there-in-the-world/

\end{document}
